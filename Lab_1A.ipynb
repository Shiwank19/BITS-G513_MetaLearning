{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_1A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzjUwlKqNwi"
      },
      "source": [
        "# Outline\n",
        "1. Introduction\n",
        "2. About Pytorch - To cover: Autograd (Important)\n",
        "3. Simple Neural Network Training (Lab 1B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7x12gCyeXBf"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "A large part of the course will require understanding of how gradient based computation works. In particular, how models can be learnt through gradients. \n",
        "\n",
        "Apart from the initial few lectures on Deep Learning, computing gradients manually for learning will become more and more hard especially with newer modules such as Convolution, etc., being introduced.\n",
        "\n",
        "Given the scenario, we sought the use of Differentiable Computing Frameworks. These frameworks utilize computation graphs which are essentially directed acyclic graphs representing operations and variables. \n",
        "\n",
        "Lets suppose we were to do the following computation as part of our learning model:\n",
        "\n",
        "$$ p = x + y $$\n",
        "$$ g = p \\times z $$\n",
        "\n",
        "A simplistic representation of the computation graph would look like:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg\">\n",
        "</p>\n",
        "\n",
        "Lets code this up in PyTorch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqe78umOr0SS",
        "outputId": "761712fa-bd53-4c69-f16f-afe07dbde67b"
      },
      "source": [
        "# Install a visualization software\n",
        "!pip install torchviz"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.7.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HGKsQbcn8QM"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "y = torch.tensor([3.], requires_grad=True)\n",
        "z = torch.tensor([4.], requires_grad=True)\n",
        "\n",
        "p = x + y\n",
        "g = p * z"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07rfaT55iCZT"
      },
      "source": [
        "We can check the values of p and g just to be sure. They should be 5 and 20 respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN4xlkO3pvkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2d1485-8176-43fd-88f7-6b21287fc341"
      },
      "source": [
        "print(f\"p: {p}\")\n",
        "print(f\"g: {g}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p: tensor([5.], grad_fn=<AddBackward0>)\n",
            "g: tensor([20.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_cqz2vwlSNi"
      },
      "source": [
        "Lets visualize the computation graph once to be sure. We will use `torchviz` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "6Ldjt5oQlZei",
        "outputId": "5e3fb182-4443-4562-9219-a643f652aa31"
      },
      "source": [
        "from torchviz import make_dot\n",
        "make_dot(g)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7ffb14548c50>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"189pt\" height=\"171pt\"\n viewBox=\"0.00 0.00 189.00 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 185,-167 185,4 -4,4\"/>\n<!-- 140716369684632 -->\n<g id=\"node1\" class=\"node\">\n<title>140716369684632</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"153.5,-21 62.5,-21 62.5,0 153.5,0 153.5,-21\"/>\n<text text-anchor=\"middle\" x=\"108\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140716369682560 -->\n<g id=\"node2\" class=\"node\">\n<title>140716369682560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"109,-85 17,-85 17,-64 109,-64 109,-85\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140716369682560&#45;&gt;140716369684632 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140716369682560&#45;&gt;140716369684632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.4308,-63.9317C76.9705,-54.6309 86.6534,-40.8597 94.5509,-29.6276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.519,-31.4914 100.4077,-21.2979 91.7928,-27.4651 97.519,-31.4914\"/>\n</g>\n<!-- 140716369684968 -->\n<g id=\"node3\" class=\"node\">\n<title>140716369684968</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-163 0,-163 0,-128 54,-128 54,-163\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140716369684968&#45;&gt;140716369682560 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140716369684968&#45;&gt;140716369682560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.8989,-127.9494C41.1277,-117.6371 47.747,-104.5824 53.1348,-93.9563\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.2607,-95.5307 57.6614,-85.0288 50.0174,-92.365 56.2607,-95.5307\"/>\n</g>\n<!-- 140716369682784 -->\n<g id=\"node4\" class=\"node\">\n<title>140716369682784</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"126,-163 72,-163 72,-128 126,-128 126,-163\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140716369682784&#45;&gt;140716369682560 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140716369682784&#45;&gt;140716369682560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90.1011,-127.9494C84.8723,-117.6371 78.253,-104.5824 72.8652,-93.9563\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.9826,-92.365 68.3386,-85.0288 69.7393,-95.5307 75.9826,-92.365\"/>\n</g>\n<!-- 140716369683624 -->\n<g id=\"node5\" class=\"node\">\n<title>140716369683624</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"181,-92 127,-92 127,-57 181,-57 181,-92\"/>\n<text text-anchor=\"middle\" x=\"154\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140716369683624&#45;&gt;140716369684632 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140716369683624&#45;&gt;140716369684632</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M141.1864,-56.6724C135.1095,-48.2176 127.8436,-38.1085 121.6618,-29.5078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.3859,-27.3009 115.7075,-21.2234 118.7018,-31.3863 124.3859,-27.3009\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Z65liViRqu"
      },
      "source": [
        "In gradient based learning, what we've done till now is considered as a forward pass. The learning part is usually during the backward pass. So what is the backward pass?\n",
        "\n",
        "The Backward pass is essentially where we compute gradients and update our parameters of the model based on those gradients, so that the output is a step closer to where its supposed to be. In this notebook, we will be concerned only with inspecting the value of the gradients rather than updating them.\n",
        "\n",
        "To compute the gradients, we can call the `backward` method on a tensor. This `backward` call will compute gradients anywhere possible on the computation graph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDijBPUciBpa"
      },
      "source": [
        "g.backward()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpbvoMzElrp7"
      },
      "source": [
        "The gradients can be computed simply in our case using backpropagation (chain rule):\n",
        "\n",
        "$$ \\frac{dg}{dp} = z = 4 $$\n",
        "\n",
        "$$ \\frac{dg}{dz} = p = 5 $$\n",
        "\n",
        "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\times \\frac{dp}{dx} = z \\times 1 = 4 $$\n",
        "\n",
        "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\times \\frac{dp}{dy} = z \\times 1 = 4 $$\n",
        "\n",
        "We can verify this by checking the gradients of all these variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN-e4S0yh90m",
        "outputId": "4d15e8a3-dd7d-4817-ad94-d67fa22a6832"
      },
      "source": [
        "print(f\"Gradient of dg wrt dz: {z.grad}\")\n",
        "print(f\"Gradient of dg wrt dx: {x.grad}\")\n",
        "print(f\"Gradient of dg wrt dy: {y.grad}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient of dg wrt dz: tensor([5.])\n",
            "Gradient of dg wrt dx: tensor([4.])\n",
            "Gradient of dg wrt dy: tensor([4.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}